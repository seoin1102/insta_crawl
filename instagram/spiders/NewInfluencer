# -*- coding: utf-8 -*-
import scrapy
from urllib.parse import urlencode
import json
import requests
from datetime import datetime
import base64
import time

# 인플루언서 이름을 입력해서 인플루언서를 새로 등록하거나 포스트를 업데이트하는 코드입니다.

start = time.time()
API = '75d53510-8a3a-49e0-b44f-0151bea39e09'
token = 'a9151baac138bde82e0bf7fd342f741109bb3687'

headers= {'Authorization': 'token ' + token,
    'Accept': 'application/json',
    'Content-Type': 'application/json;charset=UTF-8'
    }
   
# api의 인플루언서 리스트를 가져옴.
def influencer_list():
    url ='https://api.staging.dabi-api.com/api/influencer/'
    user_list = []
    i=0
    while 1:
        response = requests.get(url+'?offset='+str(i), headers=headers)
        data = response.json()
        result = data['results']
        for i2 in range(len(result)):
            user_list.append(result[i2]['insta_id']) 
        i+=20
        if data['next']==None:
            break   
    return user_list 

# influencer_list에 인플루언서 리스트가 저장됨. 
influencer_list = influencer_list()

# 인플루언서의 pk를 불러옴.
def influencer_pk(influencer):
    url ='https://api.staging.dabi-api.com/api/influencer/'
    i=0
    while 1:
        response = requests.get(url+'?offset='+str(i), headers=headers)
        data = response.json()
        result = data['results']
        
        for i2 in range(len(result)):
            if influencer == result[i2]['insta_id']:
                return result[i2]['pk']    
        i+=20
        if data['next']==None:
            break   

# 새롭게 등록하거나 업데이트하고자 하는 인플루언서 이름를 입력
username = input("insta_id: ")

# 현재 DB에 저장되어있는 포스트들의 주소를 리스트로 저장. 
def check_post(influencer_id):
    pk = influencer_pk(influencer_id)
    response = requests.get('https://api.staging.dabi-api.com/api/influencer/'+str(pk)+'/feedback/', headers=headers)
    data = response.json()
    result = data['results']
    post_list=[]
    for i in range(len(result)):
        post_list.append(result[i]['post_url'])
       
    return post_list

# 프록시 주소로 변경한 인스타그램 페이지를 만듬.
def get_url(url):
    payload = {'api_key': API, 'proxy': 'residential', 'timeout': '20000', 'url': url}
    proxy_url = 'https://api.webscraping.ai/html?' + urlencode(payload)
    return proxy_url

class PostSpider(scrapy.Spider):
    name = 'NewNew'
    allowed_domains = ['api.webscraping.ai']
    custom_settings = {'CONCURRENT_REQUESTS_PER_DOMAIN': 5,  'FEED_URI' : f"{username}.json"}    

    # 찾고자 하는 인플루언서의 인스타그램 페이지를 호출.
    def start_requests(self):
        url = f'https://www.instagram.com/{username}'
        yield scrapy.Request(get_url(url), callback=self.parse)

    def parse(self, response):
        x = response.xpath("//script[starts-with(.,'window._sharedData')]/text()").extract_first()
        json_string = "{" + x.strip().split('= {')[1][:-1]
        data = json.loads(json_string)
        
        # 사용자 id, 포스트 주소, 포스트 게시 날짜, 좋아요 수, 댓글 수, 캡션을 json으로 파싱
        user_id = data['entry_data']['ProfilePage'][0]['graphql']['user']['id']
        username = data['entry_data']['ProfilePage'][0]['graphql']['user']['username']
        next_page_bool = \
            data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media']['page_info'][
                'has_next_page']
        edges = data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media']['edges']
        edge=[]
        image_url = data['entry_data']['ProfilePage'][0]['graphql']['user']['profile_pic_url']
        base64_bytes = base64.b64encode(requests.get(image_url).content)
        image_url = base64_bytes.decode('utf-8')
        
        #  새로운 인플루언서를 rest api의 post를 이용해서 저장. 
        requests.post('https://api.staging.dabi-api.com/api/influencer/', json={"insta_id": username, "profile_image": image_url}, headers=headers)
        
        # 이미 DB에 있는 저장되어있는 포스트는 걸러냄. 
        for i in edges:
            url = 'https://www.instagram.com/p/' + i['node']['shortcode']
            if url not in check_post(username):
                edge.append(i)

        # 걸러진 새로운 포스트들을 크롤.            
        for i in edge:
            url = 'https://www.instagram.com/p/' + i['node']['shortcode']
            image_url = i['node']['thumbnail_resources'][-1]['src']
            base64_bytes = base64.b64encode(requests.get(image_url).content)
            image_url = base64_bytes.decode('utf-8')
            date_posted_timestamp = i['node']['taken_at_timestamp']
            date_posted_human = datetime.fromtimestamp(date_posted_timestamp).strftime("%Y-%m-%d")
        
            captions = ""
            
            if i['node']['edge_media_to_caption']:
                for i2 in i['node']['edge_media_to_caption']['edges']:
                    captions += i2['node']['text'] 
                captions = [i.replace('\n','') for i in captions]     
                captions = ''.join(captions)   
            item = {'post_url': url, 'post_thumb_image':image_url ,'post_taken_at_timestamp': date_posted_human,
                    'post_description': captions} 
            
            # 인플루언서의 pk를 불러옴.    
            pk= influencer_pk(username)        
            pk_url = f'https://api.staging.dabi-api.com/api/influencer/{pk}/feedback/'
            requests.post(pk_url, json={"post_url": url, "post_thumb_image":image_url ,"post_taken_at_timestamp": date_posted_human,
                    "post_description": captions}, headers=headers)    
            yield item

        # 다음 페이지가 있다면 다음 페이지 주소를 받음.      
        if next_page_bool:
            cursor = data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media']['page_info'][
                    'end_cursor']
            di = {'id': user_id, 'first': 12, 'after': cursor}
            print(di)
            params = {'query_hash': 'e769aa130647d2354c40ea6a439bfc08', 'variables': json.dumps(di)}
            url = 'https://www.instagram.com/graphql/query/?' + urlencode(params)
            
         
            yield scrapy.Request(get_url(url), callback=self.parse_pages, meta={'pages_di': di})

    def parse_pages(self, response):
        di = response.meta['pages_di']
        data = json.loads(response.text)
        edges = data['data']['user']['edge_owner_to_timeline_media']['edges']
        edge=[]
        # 이미 DB에 있는 저장되어있는 포스트는 걸러냄. 
        for i in edges:
            url = 'https://www.instagram.com/p/' + i['node']['shortcode']
            username = i['node']['owner']['username']
            if url not in check_post(username):
                edge.append(i)
        # 걸러진 새로운 포스트들을 크롤.        
        for i in edge:       
            url = 'https://www.instagram.com/p/' + i['node']['shortcode'] 
            image_url = i['node']['thumbnail_resources'][-1]['src']
            base64_bytes = base64.b64encode(requests.get(image_url).content)
            image_url = base64_bytes.decode('utf-8')
            date_posted_timestamp = i['node']['taken_at_timestamp']
            captions = ""
            
            if i['node']['edge_media_to_caption']:
                for i2 in i['node']['edge_media_to_caption']['edges']:
                    captions += i2['node']['text']
                captions = [i.replace('\n','') for i in captions] 
                captions = ''.join(captions)            
            date_posted_human = datetime.fromtimestamp(date_posted_timestamp).strftime("%Y-%m-%d")
            item = {'post_url': url, 'post_thumb_image':image_url ,'post_taken_at_timestamp': date_posted_human,
                    'post_description': captions}
            
            # 인플루언서의 pk를 불러옴.            
            pk= influencer_pk(username)        
            pk_url = f'https://api.staging.dabi-api.com/api/influencer/{pk}/feedback/'
            # 크롤한 데이터들을 rest api에 post함.
            requests.post(pk_url, json={"post_url": url, "post_thumb_image":image_url ,"post_taken_at_timestamp": date_posted_human,
                    "post_description": captions}, headers=headers)            
            yield item

        next_page_bool = data['data']['user']['edge_owner_to_timeline_media']['page_info']['has_next_page']
        if next_page_bool:
            cursor = data['data']['user']['edge_owner_to_timeline_media']['page_info']['end_cursor']
            di['after'] = cursor
            params = {'query_hash': 'e769aa130647d2354c40ea6a439bfc08', 'variables': json.dumps(di)}
            url = 'https://www.instagram.com/graphql/query/?' + urlencode(params)
            yield scrapy.Request(get_url(url), callback=self.parse_pages, meta={'pages_di': di})

   

    
